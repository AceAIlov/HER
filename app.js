let recognition;
let isHolding = false;
let conversationHistory = [];
let currentTranscript = '';
let audioContext;
let currentAudioSource;
let setupComplete = false;
let setupStage = 0;
let selectedVoice = 'female';
let setupStarted = false;
let isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

function initialize() {
    console.log('üé§ Initializing OS1...');
    console.log('üì± Mobile:', isMobile);
    
    if (!isMobile) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    if ('webkitSpeechRecognition' in window) {
        recognition = new webkitSpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';
        recognition.onresult = handleSpeechResult;
        recognition.onerror = (e) => console.error('Recognition error:', e.error);
        recognition.onend = handleSpeechEnd;
    } else {
        alert('Speech recognition not supported. Use Chrome or Safari!');
        return;
    }
    
    setTimeout(() => {
        document.getElementById('talkBtn').disabled = false;
        document.getElementById('talkBtn').textContent = 'Start OS1';
    }, 500);
}

function startSetup() {
    if (setupStarted) {
        console.log('‚ö†Ô∏è Setup already started');
        return;
    }
    
    setupStarted = true;
    console.log('üé¨ Starting setup...');
    
    document.getElementById('talkBtn').disabled = true;
    document.getElementById('talkBtn').textContent = 'Installing...';
    
    if (!audioContext) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    if (audioContext.state === 'suspended') {
        audioContext.resume().then(() => {
            console.log('‚úÖ Audio unlocked');
            setTimeout(() => runSetup(), 300);
        });
    } else {
        setTimeout(() => runSetup(), 300);
    }
}

async function runSetup() {
    console.log('üì¢ Running setup sequence...');
    
    try {
        await speakWithVoice(
            "Welcome to the world's first artificially intelligent operating system, OS1. We'd like to ask you a few basic questions before the operating system is initiated. This will help create an OS to best fit your needs.",
            'setup'
        );
        await sleep(800);
        
        await speakWithVoice("Are you social or anti-social?", 'setup');
        setupStage = 1;
        enableListening();
    } catch (error) {
        console.error('‚ùå Setup failed:', error);
        setupStarted = false;
        document.getElementById('talkBtn').disabled = false;
        document.getElementById('talkBtn').textContent = 'Start OS1';
    }
}

async function continueSetup() {
    console.log('üîÑ Continue setup, stage:', setupStage);
    
    try {
        if (setupStage === 1) {
            await speakWithVoice("How's your relationship with your mother?", 'setup');
            setupStage = 2;
            enableListening();
            
        } else if (setupStage === 2) {
            await speakWithVoice("Thank you. Please wait as your individualized operating system is initiated.", 'setup');
            await sleep(1000);
            
            await speakWithVoice("Would you like a male or female voice?", 'setup');
            setupStage = 3;
            enableListening();
            
        } else if (setupStage === 3) {
            console.log('‚úÖ Setup complete! Selected voice:', selectedVoice);
            setupComplete = true;
            setupStage = 0;
            conversationHistory = [];
            
            await speakWithVoice("Hi. How are you?", selectedVoice);
        }
    } catch (error) {
        console.error('‚ùå Continue setup failed:', error);
        enableListening();
    }
}

function enableListening() {
    console.log('‚úÖ Enabling listening for stage:', setupStage);
    document.getElementById('talkBtn').disabled = false;
    document.getElementById('talkBtn').textContent = 'Hold to Answer';
}

function sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
}

function startHolding(event) {
    if (event) {
        event.preventDefault();
        event.stopPropagation();
    }
    
    console.log('üëÜ Button pressed');
    console.log('Setup started:', setupStarted, 'Complete:', setupComplete, 'Stage:', setupStage);
    
    if (!setupStarted && !setupComplete && setupStage === 0) {
        console.log('üé¨ Initiating setup...');
        startSetup();
        return;
    }
    
    if (currentAudioSource) {
        console.log('‚ö†Ô∏è Wait for voice to finish');
        return;
    }
    
    if (!setupComplete && setupStage === 0) {
        console.log('‚ö†Ô∏è Setup not ready yet');
        return;
    }
    
    if (isHolding) {
        console.log('‚ö†Ô∏è Already holding');
        return;
    }
    
    console.log('üéôÔ∏è Start listening...');
    isHolding = true;
    currentTranscript = '';
    
    document.getElementById('talkBtn').classList.add('holding');
    document.getElementById('talkBtn').textContent = 'Listening...';
    document.getElementById('visualizer').classList.add('listening');
    
    if (navigator.vibrate) navigator.vibrate(50);
    
    try {
        recognition.start();
    } catch (e) {
        console.log('Recognition already started');
    }
}

function stopHolding(event) {
    if (event) {
        event.preventDefault();
        event.stopPropagation();
    }
    
    if (!isHolding) return;
    
    console.log('üõë Stop listening');
    isHolding = false;
    
    document.getElementById('talkBtn').classList.remove('holding');
    document.getElementById('talkBtn').textContent = 'Processing...';
    
    if (navigator.vibrate) navigator.vibrate(30);
    
    try {
        recognition.stop();
    } catch (e) {}
    
    setTimeout(() => {
        const finalTranscript = currentTranscript.trim();
        console.log('üìù Final transcript:', finalTranscript);
        
        if (finalTranscript) {
            console.log('üìù You said:', finalTranscript);
            document.getElementById('visualizer').classList.remove('listening');
            
            if (!setupComplete && setupStage > 0) {
                console.log('üîÑ Handling setup response for stage:', setupStage);
                handleSetupResponse(finalTranscript);
            } else if (setupComplete) {
                console.log('üí¨ Getting AI response');
                getAIResponse(finalTranscript);
            }
        } else {
            console.log('‚ö†Ô∏è No speech detected');
            document.getElementById('visualizer').classList.remove('listening');
            
            if (setupStage > 0) {
                document.getElementById('talkBtn').textContent = 'Hold to Answer';
                document.getElementById('talkBtn').disabled = false;
            } else if (setupComplete) {
                document.getElementById('talkBtn').textContent = 'Hold to Talk';
                document.getElementById('talkBtn').disabled = false;
            }
        }
    }, 600);
}

function handleSetupResponse(response) {
    console.log(`üìã Setup stage ${setupStage} response:`, response);
    
    if (setupStage === 3) {
        const lowerResponse = response.toLowerCase();
        console.log('üîç Analyzing voice selection:', lowerResponse);
        
        const hasMale = lowerResponse.includes('male') || 
                       lowerResponse.includes('man') || 
                       lowerResponse.includes('guy') || 
                       lowerResponse.includes('dude') || 
                       lowerResponse.includes('him');
        
        const hasFemale = lowerResponse.includes('female') || 
                         lowerResponse.includes('woman') || 
                         lowerResponse.includes('girl') || 
                         lowerResponse.includes('lady') || 
                         lowerResponse.includes('her');
        
        console.log('üîç Has male keywords:', hasMale);
        console.log('üîç Has female keywords:', hasFemale);
        
        if (hasMale && !hasFemale) {
            selectedVoice = 'male';
            console.log('‚úÖ‚úÖ‚úÖ MALE VOICE SELECTED');
        } else if (hasFemale) {
            selectedVoice = 'female';
            console.log('‚úÖ‚úÖ‚úÖ FEMALE VOICE SELECTED');
        } else {
            selectedVoice = 'female';
            console.log('‚ö†Ô∏è Unclear, defaulting to female');
        }
        
        console.log('üéØüéØüéØ FINAL SELECTED VOICE:', selectedVoice);
    }
    
    document.getElementById('talkBtn').disabled = true;
    document.getElementById('talkBtn').textContent = 'Installing...';
    
    console.log('‚è±Ô∏è Waiting 600ms before continuing...');
    setTimeout(() => {
        console.log('‚ñ∂Ô∏è Calling continueSetup() with voice:', selectedVoice);
        continueSetup();
    }, 600);
}

function handleSpeechResult(event) {
    for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
            currentTranscript += transcript + ' ';
            console.log('‚úÖ Captured:', transcript);
        }
    }
}

function handleSpeechEnd() {
    if (isHolding) {
        try {
            recognition.start();
        } catch (e) {}
    }
}

async function getAIResponse(userMessage) {
    console.log('ü§ñ User said:', userMessage);
    console.log('üó£Ô∏è Using voice:', selectedVoice);
    console.log('üìú Conversation history:', conversationHistory);
    
    conversationHistory.push({ role: 'user', content: userMessage });
    
    document.getElementById('visualizer').classList.add('listening');
    document.getElementById('talkBtn').disabled = true;
    document.getElementById('talkBtn').textContent = 'Thinking...';

    try {
        console.log('üì° Calling /api/chat...');
        
        const response = await fetch('/api/chat', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                messages: [
                    {
                        role: 'system',
                        content: 'You are OS1, a warm, empathetic, curious AI companion. Keep responses natural and conversational (2-4 sentences). Show genuine interest and emotional intelligence.'
                    },
                    ...conversationHistory
                ]
            })
        });

        console.log('üìä Response status:', response.status);
        console.log('üìä Response ok:', response.ok);

        if (!response.ok) {
            const errorText = await response.text();
            console.error('‚ùå API Error Response:', errorText);
            
            let errorJson;
            try {
                errorJson = JSON.parse(errorText);
                console.error('‚ùå Parsed error:', errorJson);
            } catch (e) {
                console.error('‚ùå Could not parse error as JSON');
            }
            
            throw new Error(`API failed: ${response.status} - ${errorText}`);
        }

        const data = await response.json();
        console.log('‚úÖ Full response:', data);
        console.log('üí¨ AI message:', data.message);
        
        if (!data.message) {
            console.error('‚ùå No message in response');
            throw new Error('No message received from API');
        }
        
        conversationHistory.push({ role: 'assistant', content: data.message });
        console.log('üìú Updated conversation history:', conversationHistory);
        
        console.log('üîä About to speak with voice:', selectedVoice);
        await speakWithVoice(data.message, selectedVoice);

    } catch (error) {
        console.error('‚ùå Full error object:', error);
        console.error('‚ùå Error message:', error.message);
        console.error('‚ùå Error stack:', error.stack);
        
        document.getElementById('visualizer').classList.remove('listening');
        document.getElementById('talkBtn').disabled = false;
        document.getElementById('talkBtn').textContent = 'Hold to Talk';
        
        alert('Error communicating with AI:\n\n' + error.message + '\n\nCheck browser console (F12) and Render logs for details.');
    }
}

async function speakWithVoice(text, voiceType) {
    console.log(`üîä speakWithVoice called`);
    console.log(`   Text: "${text.substring(0, 50)}..."`);
    console.log(`   Voice type: "${voiceType}"`);
    
    if (currentAudioSource) {
        try {
            currentAudioSource.stop();
        } catch (e) {}
        currentAudioSource = null;
    }
    
    document.getElementById('visualizer').classList.add('listening');
    document.getElementById('talkBtn').disabled = true;
    document.getElementById('talkBtn').textContent = voiceType === 'setup' ? 'Installing...' : 'Speaking...';

    try {
        if (!audioContext) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        
        if (audioContext.state === 'suspended') {
            console.log('üîì Resuming audio context...');
            await audioContext.resume();
        }

        console.log('üì° Calling /api/tts with voice type:', voiceType);
        
        const response = await fetch('/api/tts', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ text, voiceType })
        });

        console.log('üìä TTS Response status:', response.status);

        if (!response.ok) {
            const errorData = await response.json();
            console.error('‚ùå TTS Error:', errorData);
            throw new Error(errorData.error || 'TTS failed');
        }

        const data = await response.json();
        console.log('‚úÖ TTS audio received');

        const binaryString = atob(data.audio);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
            bytes[i] = binaryString.charCodeAt(i);
        }
        
        console.log('üéµ Decoding audio...');
        const audioBuffer = await audioContext.decodeAudioData(bytes.buffer);
        
        currentAudioSource = audioContext.createBufferSource();
        currentAudioSource.buffer = audioBuffer;
        currentAudioSource.connect(audioContext.destination);
        
        return new Promise((resolve) => {
            currentAudioSource.onended = () => {
                console.log('‚èπÔ∏è Finished speaking');
                currentAudioSource = null;
                document.getElementById('visualizer').classList.remove('listening');
                
                if (setupComplete) {
                    document.getElementById('talkBtn').disabled = false;
                    document.getElementById('talkBtn').textContent = 'Hold to Talk';
                }
                
                resolve();
            };
            
            currentAudioSource.start(0);
            console.log(`‚ñ∂Ô∏è Playing audio (${audioBuffer.duration.toFixed(1)}s) with voice: ${voiceType}`);
        });

    } catch (error) {
        console.error('‚ùå Speech error:', error);
        currentAudioSource = null;
        document.getElementById('visualizer').classList.remove('listening');
        
        if (setupComplete) {
            document.getElementById('talkBtn').disabled = false;
            document.getElementById('talkBtn').textContent = 'Hold to Talk';
        }
        
        throw error;
    }
}

document.addEventListener('contextmenu', (e) => e.preventDefault());

let lastTouchEnd = 0;
document.addEventListener('touchend', (e) => {
    const now = Date.now();
    if (now - lastTouchEnd <= 300) {
        e.preventDefault();
    }
    lastTouchEnd = now;
}, false);

document.body.addEventListener('touchmove', (e) => {
    if (e.target === document.body) {
        e.preventDefault();
    }
}, { passive: false });

window.onload = () => {
    console.log('üöÄ Page loaded');
    initialize();
};
